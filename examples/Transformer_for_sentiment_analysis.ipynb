{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TUTORIAL USING ROMANIAN BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post we will take a pretrained model and we will create a classifiers on top of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install transformers tokenizers pytorch-lightning torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import *\n",
    "import logging\n",
    "import os\n",
    "from functools import lru_cache\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "from imbalanced_sampler import ImbalancedDatasetSampler\n",
    "from argparse import Namespace\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pre-Trained BERT ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.tokenization_utils:Model name 'dumitrescustefan/bert-base-romanian-cased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming 'dumitrescustefan/bert-base-romanian-cased-v1' is a path or url to a directory containing tokenizer files.\n",
      "INFO:transformers.tokenization_utils:Didn't find file dumitrescustefan/bert-base-romanian-cased-v1/added_tokens.json. We won't load it.\n",
      "INFO:transformers.tokenization_utils:Didn't find file dumitrescustefan/bert-base-romanian-cased-v1/special_tokens_map.json. We won't load it.\n",
      "INFO:transformers.tokenization_utils:Didn't find file dumitrescustefan/bert-base-romanian-cased-v1/tokenizer_config.json. We won't load it.\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/dumitrescustefan/bert-base-romanian-cased-v1/vocab.txt from cache at /home/mihai/.cache/torch/transformers/8f484b8d8325d2eaf7ce7a1b76c3039ab5dac7f66e990510fec72516f38606eb.549839faf324dd95ad2a010a98d79e9ba649e1902543250d7f303dbe5911c2bb\n",
      "INFO:transformers.tokenization_utils:loading file None\n",
      "INFO:transformers.tokenization_utils:loading file None\n",
      "INFO:transformers.tokenization_utils:loading file None\n",
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/dumitrescustefan/bert-base-romanian-cased-v1/config.json from cache at /home/mihai/.cache/torch/transformers/16f881d81e2cc1a24df63c9281832f2a67e993ce53cc89ffb92c86e198fa0fce.0238ce9e237411721966c916e09e62c904acebf01de9678d4263c97d3d95ec18\n",
      "INFO:transformers.configuration_utils:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/dumitrescustefan/bert-base-romanian-cased-v1/pytorch_model.bin from cache at /home/mihai/.cache/torch/transformers/43f08c09359d4992b29f1910631e4813ab686ac589a0ef4ebb74e95217aeb8af.aeb64baccc31c441972d0f4a7d5306c3ccd091a9db58162dae7442f024699db7\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dumitrescustefan/bert-base-romanian-cased-v1\")\n",
    "config = BertConfig.from_pretrained(\"dumitrescustefan/bert-base-romanian-cased-v1\", output_hidden_states=True)\n",
    "model = AutoModel.from_pretrained(\"dumitrescustefan/bert-base-romanian-cased-v1\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(50000, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentence is: ['[CLS]', 'am', 'imprumut', '##at', 'cartea', 'acesta', '.', '[SEP]'] and it has the following ids : [2, 474, 48617, 368, 4435, 1330, 18, 3]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Am împrumutat cartea acesta.\"\n",
    "mark_sentence = \"[CLS]\" + sentence + \"[SEP]\"\n",
    "tokenized_sentence = tokenizer.tokenize(mark_sentence)\n",
    "enc_sentence = tokenizer.encode(sentence)\n",
    "print(\"Tokenized sentence is: {} and it has the following ids : {}\".format(tokenized_sentence, enc_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['controlul',\n",
       " 'schimba',\n",
       " '##alia',\n",
       " 'împrum',\n",
       " '##năsti',\n",
       " 'acordat',\n",
       " 'aste',\n",
       " '##ction',\n",
       " '##atia',\n",
       " '2003',\n",
       " 'mașini',\n",
       " 'PNL',\n",
       " 'dans',\n",
       " 'anexa',\n",
       " '##cilor',\n",
       " '##osc',\n",
       " 'învin',\n",
       " 'Transilv',\n",
       " 'oa',\n",
       " '##anților']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenizer.vocab.keys())[5000:5020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\t2\n",
      "printesa\t2\n",
      "a\t42240\n",
      "sar\t69\n",
      "##utat\t2610\n",
      "broa\t3452\n",
      "##sca\t38130\n",
      "dar\t2298\n",
      "aceasta\t592\n",
      "nu\t1239\n",
      "s\t411\n",
      "-\t87\n",
      "a\t17\n",
      "transformat\t69\n",
      "in\t8057\n",
      "print\t402\n",
      ".\t24930\n",
      "nu\t18\n",
      "am\t411\n",
      "gasit\t474\n",
      "cheia\t10177\n",
      "ca\t12685\n",
      "sa\t416\n",
      "deschid\t446\n",
      "broa\t14180\n",
      "##sca\t38130\n",
      "de\t2298\n",
      "la\t363\n",
      "usa\t392\n",
      ".\t19386\n",
      "[SEP]\t18\n"
     ]
    }
   ],
   "source": [
    "##create new example with omonymns\n",
    "sentences = \"Prințesa a sărutat broasca dar aceasta nu s-a transformat in prinț.\" \\\n",
    "            \"Nu am găsit cheia ca să deschid broasca de la ușă.\"\n",
    "\n",
    "# Add the special tokens.\n",
    "mark_sentences = \"[CLS] \" + sentences + \" [SEP]\"\n",
    "\n",
    "# Tokenize sentences into tokens\n",
    "tokenized_sentences = tokenizer.tokenize(mark_sentences)\n",
    "\n",
    "# Map tokens to ids in vocabulary\n",
    "ids_sentences = tokenizer.encode(mark_sentences)\n",
    "\n",
    "for tok, id in zip(tokenized_sentences, ids_sentences):\n",
    "    print(\"{}\\t{}\".format(tok, id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "ids_segments = [1] * len(ids_sentences)\n",
    "\n",
    "print(ids_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.tensor(ids_sentences).unsqueeze(0)  # Batch size 1\n",
    "segments_tensor = torch.tensor(ids_segments).unsqueeze(0) # batch size 1 \n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_tensor, segments_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Understand output layers\n",
    "last_hidden_states = outputs[0]\n",
    "# tensor of shape (batch_size, sequence_length, hidden_size)\n",
    "\n",
    "pooler_output = outputs[1]\n",
    "# tensor of shape (batch_size, hidden_size)\n",
    "\n",
    "hidden_states = outputs[2]\n",
    "# list of tensors of shape (batch_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 13\n",
      "Number of batches: 1\n",
      "Number of tokens: 33\n",
      "Number of hidden units: 768\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of layers:\", len(hidden_states))\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(hidden_states[layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i]))\n",
    "token_i = 0\n",
    "\n",
    "print (\"Number of hidden units:\", len(hidden_states[layer_i][batch_i][token_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For the 5th token in our sentence, select its feature values from layer 5.\n",
    "token_i = 3\n",
    "layer_i = 7\n",
    "vec = hidden_states[layer_i][batch_i][token_i]\n",
    "\n",
    "# Plot the values as a histogram to show their distribution.\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(vec, bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 33, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the tensors for all layers. We use `stack` here to\n",
    "# create a new dimension in the tensor.\n",
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "token_embeddings = token_embeddings.squeeze(1)\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embedding representations for words and sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 16 x 4608\n"
     ]
    }
   ],
   "source": [
    "def get_tokkens_embeddings_concat(sentence, max_length=16):\n",
    "    # list with tokens embeddings for each token\n",
    "    token_vecs_concat = []\n",
    "    \n",
    "    ids_sentences = tokenizer.encode(sentence, add_special_tokens=True,\n",
    "                                    pad_to_max_length=True,\n",
    "                                    max_length=max_length)\n",
    "    ids_segments = [1] * len(ids_sentences)\n",
    "    input_tensor = torch.tensor(ids_sentences).unsqueeze(0)  # Batch size 1\n",
    "    segments_tensor = torch.tensor(ids_segments).unsqueeze(0) # batch size 1 \n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, _, hidden_states = model(input_tensor, segments_tensor)\n",
    "                                    \n",
    "    token_embeddings = torch.stack(hidden_states, dim=0).squeeze(1).permute(1,0,2)\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "        cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4], token[-5], token[-6]), dim=0)\n",
    "\n",
    "        token_vecs_concat.append(cat_vec)\n",
    "        \n",
    "    return token_vecs_concat\n",
    "\n",
    "tokens_emb = get_tokkens_embeddings_concat(\"Eu am fost in Vama Veche.\")\n",
    "print ('Shape is: %d x %d' % (len(tokens_emb), len(tokens_emb[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 16 x 768\n"
     ]
    }
   ],
   "source": [
    "def get_tokkens_embeddings_summ(sentence, max_length=16):\n",
    "    token_vecs_summ = []\n",
    "\n",
    "    ids_sentences = tokenizer.encode(sentence, add_special_tokens=True,\n",
    "                                    pad_to_max_length=True,\n",
    "                                    max_length=max_length)\n",
    "    ids_segments = [1] * len(ids_sentences)\n",
    "    input_tensor = torch.tensor(ids_sentences).unsqueeze(0)  # Batch size 1\n",
    "    segments_tensor = torch.tensor(ids_segments).unsqueeze(0) # batch size 1 \n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, _, hidden_states = model(input_tensor, segments_tensor)\n",
    "                                    \n",
    "    token_embeddings = torch.stack(hidden_states, dim=0).squeeze(1).permute(1,0,2)\n",
    "    \n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "        sum_vec = torch.sum(token[-6:], dim=0)\n",
    "\n",
    "        token_vecs_summ.append(sum_vec)\n",
    "\n",
    "    return token_vecs_summ\n",
    "    \n",
    "tokens_emb = get_tokkens_embeddings_summ(\"Aceasta este o propozitie.\")\n",
    "print ('Shape is: %d x %d' % (len(tokens_emb), len(tokens_emb[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_similarity(tokens_emb, idx1, idx2):\n",
    "    \n",
    "    tok1_emb = tokens_emb[idx1]\n",
    "    tok2_emb = tokens_emb[idx2]\n",
    "\n",
    "    output = cosine(tok1_emb, tok2_emb)\n",
    "    \n",
    "    return 1-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.754744827747345\n",
      "0.588172972202301\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"Am mers pe lac si am inotat in lac.\"\n",
    "sentence2 = \"Lebedele inotau pe lac si am dat cu lac pe unghii.\"\n",
    "tokens_emb1 = get_tokkens_embeddings_concat(sentence1)\n",
    "tokens_emb2 = get_tokkens_embeddings_concat(sentence2)\n",
    "\n",
    "print(tokens_similarity(tokens_emb1, 3, 9))\n",
    "print(tokens_similarity(tokens_emb2, 6, 11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Embeddings ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embeddings(sentence, max_length=16):\n",
    "    \n",
    "    ids_sentences = tokenizer.encode(sentence, add_special_tokens=True,\n",
    "                                    pad_to_max_length=True,\n",
    "                                    max_length=max_length)\n",
    "    ids_segments = [1] * len(ids_sentences)\n",
    "    input_tensor = torch.tensor(ids_sentences).unsqueeze(0)  # Batch size 1\n",
    "    segments_tensor = torch.tensor(ids_segments).unsqueeze(0) # batch size 1 \n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, _, hidden_states = model(input_tensor, segments_tensor)\n",
    "    \n",
    "    token_vecs = hidden_states[12][0]\n",
    "\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "    \n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2):\n",
    "    \n",
    "    sent1_emb = get_sentence_embeddings(sent1)\n",
    "    sent2_emb = get_sentence_embeddings(sent2)\n",
    "    \n",
    "    output = cosine(sent1_emb, sent2_emb)\n",
    "    \n",
    "    return 1-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8724686503410339\n",
      "0.7341930866241455\n",
      "0.7526881098747253\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"Am fost la doctor la un control\"\n",
    "sentence2 = \"Medicul a operat in salon\"\n",
    "sentence3 = \"Nu am fost niciodata atat de vesela\"\n",
    "\n",
    "print(sentence_similarity(sentence1, sentence2))\n",
    "print(sentence_similarity(sentence2, sentence3))\n",
    "print(sentence_similarity(sentence1, sentence3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SENTENCE CLASSIFICATION WITH BERT ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df = pd.read_csv(\"./ro/train.csv\")\n",
    "df.dropna(inplace=True)\n",
    "df.label = df.label.astype('category')\n",
    "train_df = df.iloc[:15000]\n",
    "validation_df = df.iloc[15000:]\n",
    "test_df = pd.read_csv(\"./ro/test.csv\")\n",
    "test_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2651.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5517.340626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>768.211096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4187.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4852.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5516.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6183.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6846.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             index\n",
       "count  2651.000000\n",
       "mean   5517.340626\n",
       "std     768.211096\n",
       "min    4187.000000\n",
       "25%    4852.500000\n",
       "50%    5516.000000\n",
       "75%    6183.500000\n",
       "max    6846.000000"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Training Label Distribution')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df.label.hist()\n",
    "plt.title(\"Training Label Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>acest document mi-a deschis cu adevarat ochii la ceea ce oamenii din afara statelor unite s-au gandit la atacurile din 11 septembrie. acest film a fost construit in mod expert si prezinta acest dezastru ca fiind mai mult decat un atac asupra pamantului american. urmarile acestui dezastru sunt previzionate din multe tari si perspective diferite. cred ca acest film ar trebui sa fie mai bine distribuit pentru acest punct. de asemenea, el ajuta in procesul de vindecare sa vada in cele din urma altceva decat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>tine mancarea rece. ce altceva ii mai trebuie?\\nam frigiderul de vreun an, utilizare continua.\\nzgomotul e decent spre scazut daca esti in aceeasi incapere cu el. nu il auzi din dormitory cu usile inchise.\\ne un frigider ce sa zici despre el?\\no sa tina mancarea rece, la temperaturile indicate. recomand sa spalati bine dispenserul de lichide dupa utilizare ca sa nu se contamineze apa. da asta e de bun simt: d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>excelent\\nrecomand!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ca un rocker imbatranit, acest film mentioneaza heep and quo - cele doua trupe preferate ale mele vreodata - dar cu o distributie incredibila (toata lumea) - si povestea fantastica - imi place doar aceasta piesa de geniu creativ. nu-l pot recomanda mai mult - si mick jones a adaugat atat de mult (leaderul si scriitorul principal alaturi de cel mai mare cantaret rock - lou gramm) - am vazut aceasta mare lucrare mai mult de 10 ori - bill nighy - ce voce - si jimmy nail - talentul risipeste de la fiecare p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ei bine, a facut o groaza veche si foarte intunecata in casa. setare buna, care include personajul lui poe, el insusi, facand referire la poveste intr-un pub din londra. desi de aici este destul de mult un tip care a luat indrazneala sa viziteze casa intr-o noapte speciala care ruleaza din camera in camera, fie cauta sau evita oameni, este inca cea mai placuta. in plus, avem incantatoarea si enigmatica barbara steele. exista un dialog din lemn si niste biti si boboci inexplicabili, dar este atmosfera su...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  \\\n",
       "0      0   \n",
       "1      1   \n",
       "2      2   \n",
       "3      3   \n",
       "4      4   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              text  \\\n",
       "0  acest document mi-a deschis cu adevarat ochii la ceea ce oamenii din afara statelor unite s-au gandit la atacurile din 11 septembrie. acest film a fost construit in mod expert si prezinta acest dezastru ca fiind mai mult decat un atac asupra pamantului american. urmarile acestui dezastru sunt previzionate din multe tari si perspective diferite. cred ca acest film ar trebui sa fie mai bine distribuit pentru acest punct. de asemenea, el ajuta in procesul de vindecare sa vada in cele din urma altceva decat...   \n",
       "1                                                                                                     tine mancarea rece. ce altceva ii mai trebuie?\\nam frigiderul de vreun an, utilizare continua.\\nzgomotul e decent spre scazut daca esti in aceeasi incapere cu el. nu il auzi din dormitory cu usile inchise.\\ne un frigider ce sa zici despre el?\\no sa tina mancarea rece, la temperaturile indicate. recomand sa spalati bine dispenserul de lichide dupa utilizare ca sa nu se contamineze apa. da asta e de bun simt: d   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              excelent\\nrecomand!   \n",
       "3  ca un rocker imbatranit, acest film mentioneaza heep and quo - cele doua trupe preferate ale mele vreodata - dar cu o distributie incredibila (toata lumea) - si povestea fantastica - imi place doar aceasta piesa de geniu creativ. nu-l pot recomanda mai mult - si mick jones a adaugat atat de mult (leaderul si scriitorul principal alaturi de cel mai mare cantaret rock - lou gramm) - am vazut aceasta mare lucrare mai mult de 10 ori - bill nighy - ce voce - si jimmy nail - talentul risipeste de la fiecare p...   \n",
       "4  ei bine, a facut o groaza veche si foarte intunecata in casa. setare buna, care include personajul lui poe, el insusi, facand referire la poveste intr-un pub din londra. desi de aici este destul de mult un tip care a luat indrazneala sa viziteze casa intr-o noapte speciala care ruleaza din camera in camera, fie cauta sau evita oameni, este inca cea mai placuta. in plus, avem incantatoarea si enigmatica barbara steele. exista un dialog din lemn si niste biti si boboci inexplicabili, dar este atmosfera su...   \n",
       "\n",
       "  label  \n",
       "0     1  \n",
       "1     1  \n",
       "2     1  \n",
       "3     1  \n",
       "4     1  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 512\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tine mancarea rece. ce altceva ii mai trebuie?\\nam frigiderul de vreun an, utilizare continua.\\nzgomotul e decent spre scazut daca esti in aceeasi incapere cu el. nu il auzi din dormitory cu usile inchise.\\ne un frigider ce sa zici despre el?\\no sa tina mancarea rece, la temperaturile indicate. recomand sa spalati bine dispenserul de lichide dupa utilizare ca sa nu se contamineze apa. da asta e de bun simt: d'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.values[1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation function ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def mish(input):\n",
    "    '''\n",
    "    Applies the mish function element-wise:\n",
    "    mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + exp(x)))\n",
    "    See additional documentation for mish class.\n",
    "    '''\n",
    "    return input * torch.tanh(F.softplus(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mish(nn.Module):\n",
    "    '''\n",
    "    Applies the mish function element-wise:\n",
    "    mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + exp(x)))\n",
    "    Shape:\n",
    "        - Input: (N, *) where * means, any number of additional\n",
    "          dimensions\n",
    "        - Output: (N, *), same shape as the input\n",
    "    Examples:\n",
    "        >>> m = Mish()\n",
    "        >>> input = torch.randn(2)\n",
    "        >>> output = m(input)\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Init method.\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Forward pass of the function.\n",
    "        '''\n",
    "        return mish(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, model,\n",
    "                output_size,\n",
    "                embedding_size=768,\n",
    "                dropout=0.1\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(embedding_size, embedding_size)\n",
    "        self.fc2 = nn.Linear(embedding_size, output_size)\n",
    "        self.activation = Mish()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \n",
    "        ids_sentences = [tokenizer.encode(text, add_special_tokens=True,\n",
    "                                        pad_to_max_length=True,\n",
    "                                        max_length=128) for text in X]\n",
    "        input_tensor = torch.tensor(ids_sentences)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            last_hidden_state, _, hidden_states = self.model(input_tensor)\n",
    "\n",
    "        sent_emb = last_hidden_state[:, 0, :]\n",
    "\n",
    "        output = self.activation(self.dropout(self.fc1(sent_emb)))\n",
    "        output = self.dropout(self.fc2(output))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SentimentModel(model, output_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3171, -0.0678],\n",
       "        [ 0.0000, -0.0007]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing the classifier\n",
    "ids_sentences = (\"Ma simt foate rau.\",\"Ma simt foate rau.\")\n",
    "\n",
    "classifier(ids_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        super().__init__()\n",
    "        self.data = df\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data.values[idx][1], self.data.values[idx][2]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('distractie excelenta de la inceput pana la sfarsit. spectacole minunate de belushi, beach, dalton & railsback. unele rasturnari si multe scene de actiune. filmul a fost facut pentru mine! linii amuzante in scenariu, muzica buna. dalton ca seriful dur si railsback ca \"raufacator\". trebuie sa recomand acest film tuturor fanilor de actiune si aventura! 10/10',\n",
       " 1)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = SentimentDataset(train_df)\n",
    "train_dataset[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingModule(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.model = SentimentModel(model, output_size=2)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.hparams = hparams\n",
    "        \n",
    "    def step(self, batch, step_name=\"train\"):\n",
    "        X, y = batch\n",
    "        loss = self.loss(self.forward(X), y)\n",
    "        loss_key = f\"{step_name}_loss\"\n",
    "        tensorboard_logs = {loss_key: loss}\n",
    "        print({ (\"loss\" if step_name == \"train\" else loss_key): loss, 'log': tensorboard_logs,\n",
    "               \"progress_bar\": {loss_key: loss}})\n",
    "        return { (\"loss\" if step_name == \"train\" else loss_key): loss, 'log': tensorboard_logs,\n",
    "               \"progress_bar\": {loss_key: loss}}\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.model(X)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"train\")\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"valid\")\n",
    "    \n",
    "    def validation_end(self, outputs):\n",
    "        loss = torch.stack([x[\"valid_loss\"] for x in outputs]).mean()\n",
    "        return {\"valid_loss\": loss}\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"test\")\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.create_data_loader(self.hparams.train_df, shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return self.create_data_loader(self.hparams.val_df)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.create_data_loader(self.hparams.test_df)\n",
    "                \n",
    "    def create_data_loader(self, df, shuffle=False):\n",
    "        return DataLoader(\n",
    "                    SentimentDataset(df),\n",
    "                    batch_size=self.hparams.batch_size,\n",
    "                    shuffle=False\n",
    "                    )\n",
    "    \n",
    "    @lru_cache()\n",
    "    def total_steps(self):\n",
    "        return len(self.train_dataloader()) // self.hparams.accumulate_grad_batches * self.hparams.epochs\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        ## use AdamW optimizer -- faster approach to training NNs\n",
    "        ## read: https://www.fast.ai/2018/07/02/adam-weight-decay/\n",
    "        optimizer = AdamW(self.model.parameters(), lr=self.hparams.lr)\n",
    "        lr_scheduler = get_linear_schedule_with_warmup(\n",
    "                    optimizer,\n",
    "                    num_warmup_steps=self.hparams.warmup_steps,\n",
    "                    num_training_steps=self.total_steps(),\n",
    "        )\n",
    "        return [optimizer], [{\"scheduler\": lr_scheduler, \"interval\": \"step\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = Namespace(\n",
    "    train_df=train_df,\n",
    "    test_df=test_df,\n",
    "    val_df=validation_df,\n",
    "    batch_size=16,\n",
    "    warmup_steps=100,\n",
    "    epochs=1,\n",
    "    lr=lr,\n",
    "    accumulate_grad_batches=1,\n",
    ")\n",
    "module = TrainingModule(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning:GPU available: False, used: False\n",
      "INFO:lightning:\n",
      "    | Name                                                    | Type              | Params\n",
      "------------------------------------------------------------------------------------------\n",
      "0   | model                                                   | SentimentModel    | 125 M \n",
      "1   | model.model                                             | BertModel         | 124 M \n",
      "2   | model.model.embeddings                                  | BertEmbeddings    | 38 M  \n",
      "3   | model.model.embeddings.word_embeddings                  | Embedding         | 38 M  \n",
      "4   | model.model.embeddings.position_embeddings              | Embedding         | 393 K \n",
      "5   | model.model.embeddings.token_type_embeddings            | Embedding         | 1 K   \n",
      "6   | model.model.embeddings.LayerNorm                        | LayerNorm         | 1 K   \n",
      "7   | model.model.embeddings.dropout                          | Dropout           | 0     \n",
      "8   | model.model.encoder                                     | BertEncoder       | 85 M  \n",
      "9   | model.model.encoder.layer                               | ModuleList        | 85 M  \n",
      "10  | model.model.encoder.layer.0                             | BertLayer         | 7 M   \n",
      "11  | model.model.encoder.layer.0.attention                   | BertAttention     | 2 M   \n",
      "12  | model.model.encoder.layer.0.attention.self              | BertSelfAttention | 1 M   \n",
      "13  | model.model.encoder.layer.0.attention.self.query        | Linear            | 590 K \n",
      "14  | model.model.encoder.layer.0.attention.self.key          | Linear            | 590 K \n",
      "15  | model.model.encoder.layer.0.attention.self.value        | Linear            | 590 K \n",
      "16  | model.model.encoder.layer.0.attention.self.dropout      | Dropout           | 0     \n",
      "17  | model.model.encoder.layer.0.attention.output            | BertSelfOutput    | 592 K \n",
      "18  | model.model.encoder.layer.0.attention.output.dense      | Linear            | 590 K \n",
      "19  | model.model.encoder.layer.0.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "20  | model.model.encoder.layer.0.attention.output.dropout    | Dropout           | 0     \n",
      "21  | model.model.encoder.layer.0.intermediate                | BertIntermediate  | 2 M   \n",
      "22  | model.model.encoder.layer.0.intermediate.dense          | Linear            | 2 M   \n",
      "23  | model.model.encoder.layer.0.output                      | BertOutput        | 2 M   \n",
      "24  | model.model.encoder.layer.0.output.dense                | Linear            | 2 M   \n",
      "25  | model.model.encoder.layer.0.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "26  | model.model.encoder.layer.0.output.dropout              | Dropout           | 0     \n",
      "27  | model.model.encoder.layer.1                             | BertLayer         | 7 M   \n",
      "28  | model.model.encoder.layer.1.attention                   | BertAttention     | 2 M   \n",
      "29  | model.model.encoder.layer.1.attention.self              | BertSelfAttention | 1 M   \n",
      "30  | model.model.encoder.layer.1.attention.self.query        | Linear            | 590 K \n",
      "31  | model.model.encoder.layer.1.attention.self.key          | Linear            | 590 K \n",
      "32  | model.model.encoder.layer.1.attention.self.value        | Linear            | 590 K \n",
      "33  | model.model.encoder.layer.1.attention.self.dropout      | Dropout           | 0     \n",
      "34  | model.model.encoder.layer.1.attention.output            | BertSelfOutput    | 592 K \n",
      "35  | model.model.encoder.layer.1.attention.output.dense      | Linear            | 590 K \n",
      "36  | model.model.encoder.layer.1.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "37  | model.model.encoder.layer.1.attention.output.dropout    | Dropout           | 0     \n",
      "38  | model.model.encoder.layer.1.intermediate                | BertIntermediate  | 2 M   \n",
      "39  | model.model.encoder.layer.1.intermediate.dense          | Linear            | 2 M   \n",
      "40  | model.model.encoder.layer.1.output                      | BertOutput        | 2 M   \n",
      "41  | model.model.encoder.layer.1.output.dense                | Linear            | 2 M   \n",
      "42  | model.model.encoder.layer.1.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "43  | model.model.encoder.layer.1.output.dropout              | Dropout           | 0     \n",
      "44  | model.model.encoder.layer.2                             | BertLayer         | 7 M   \n",
      "45  | model.model.encoder.layer.2.attention                   | BertAttention     | 2 M   \n",
      "46  | model.model.encoder.layer.2.attention.self              | BertSelfAttention | 1 M   \n",
      "47  | model.model.encoder.layer.2.attention.self.query        | Linear            | 590 K \n",
      "48  | model.model.encoder.layer.2.attention.self.key          | Linear            | 590 K \n",
      "49  | model.model.encoder.layer.2.attention.self.value        | Linear            | 590 K \n",
      "50  | model.model.encoder.layer.2.attention.self.dropout      | Dropout           | 0     \n",
      "51  | model.model.encoder.layer.2.attention.output            | BertSelfOutput    | 592 K \n",
      "52  | model.model.encoder.layer.2.attention.output.dense      | Linear            | 590 K \n",
      "53  | model.model.encoder.layer.2.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "54  | model.model.encoder.layer.2.attention.output.dropout    | Dropout           | 0     \n",
      "55  | model.model.encoder.layer.2.intermediate                | BertIntermediate  | 2 M   \n",
      "56  | model.model.encoder.layer.2.intermediate.dense          | Linear            | 2 M   \n",
      "57  | model.model.encoder.layer.2.output                      | BertOutput        | 2 M   \n",
      "58  | model.model.encoder.layer.2.output.dense                | Linear            | 2 M   \n",
      "59  | model.model.encoder.layer.2.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "60  | model.model.encoder.layer.2.output.dropout              | Dropout           | 0     \n",
      "61  | model.model.encoder.layer.3                             | BertLayer         | 7 M   \n",
      "62  | model.model.encoder.layer.3.attention                   | BertAttention     | 2 M   \n",
      "63  | model.model.encoder.layer.3.attention.self              | BertSelfAttention | 1 M   \n",
      "64  | model.model.encoder.layer.3.attention.self.query        | Linear            | 590 K \n",
      "65  | model.model.encoder.layer.3.attention.self.key          | Linear            | 590 K \n",
      "66  | model.model.encoder.layer.3.attention.self.value        | Linear            | 590 K \n",
      "67  | model.model.encoder.layer.3.attention.self.dropout      | Dropout           | 0     \n",
      "68  | model.model.encoder.layer.3.attention.output            | BertSelfOutput    | 592 K \n",
      "69  | model.model.encoder.layer.3.attention.output.dense      | Linear            | 590 K \n",
      "70  | model.model.encoder.layer.3.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "71  | model.model.encoder.layer.3.attention.output.dropout    | Dropout           | 0     \n",
      "72  | model.model.encoder.layer.3.intermediate                | BertIntermediate  | 2 M   \n",
      "73  | model.model.encoder.layer.3.intermediate.dense          | Linear            | 2 M   \n",
      "74  | model.model.encoder.layer.3.output                      | BertOutput        | 2 M   \n",
      "75  | model.model.encoder.layer.3.output.dense                | Linear            | 2 M   \n",
      "76  | model.model.encoder.layer.3.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "77  | model.model.encoder.layer.3.output.dropout              | Dropout           | 0     \n",
      "78  | model.model.encoder.layer.4                             | BertLayer         | 7 M   \n",
      "79  | model.model.encoder.layer.4.attention                   | BertAttention     | 2 M   \n",
      "80  | model.model.encoder.layer.4.attention.self              | BertSelfAttention | 1 M   \n",
      "81  | model.model.encoder.layer.4.attention.self.query        | Linear            | 590 K \n",
      "82  | model.model.encoder.layer.4.attention.self.key          | Linear            | 590 K \n",
      "83  | model.model.encoder.layer.4.attention.self.value        | Linear            | 590 K \n",
      "84  | model.model.encoder.layer.4.attention.self.dropout      | Dropout           | 0     \n",
      "85  | model.model.encoder.layer.4.attention.output            | BertSelfOutput    | 592 K \n",
      "86  | model.model.encoder.layer.4.attention.output.dense      | Linear            | 590 K \n",
      "87  | model.model.encoder.layer.4.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "88  | model.model.encoder.layer.4.attention.output.dropout    | Dropout           | 0     \n",
      "89  | model.model.encoder.layer.4.intermediate                | BertIntermediate  | 2 M   \n",
      "90  | model.model.encoder.layer.4.intermediate.dense          | Linear            | 2 M   \n",
      "91  | model.model.encoder.layer.4.output                      | BertOutput        | 2 M   \n",
      "92  | model.model.encoder.layer.4.output.dense                | Linear            | 2 M   \n",
      "93  | model.model.encoder.layer.4.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "94  | model.model.encoder.layer.4.output.dropout              | Dropout           | 0     \n",
      "95  | model.model.encoder.layer.5                             | BertLayer         | 7 M   \n",
      "96  | model.model.encoder.layer.5.attention                   | BertAttention     | 2 M   \n",
      "97  | model.model.encoder.layer.5.attention.self              | BertSelfAttention | 1 M   \n",
      "98  | model.model.encoder.layer.5.attention.self.query        | Linear            | 590 K \n",
      "99  | model.model.encoder.layer.5.attention.self.key          | Linear            | 590 K \n",
      "100 | model.model.encoder.layer.5.attention.self.value        | Linear            | 590 K \n",
      "101 | model.model.encoder.layer.5.attention.self.dropout      | Dropout           | 0     \n",
      "102 | model.model.encoder.layer.5.attention.output            | BertSelfOutput    | 592 K \n",
      "103 | model.model.encoder.layer.5.attention.output.dense      | Linear            | 590 K \n",
      "104 | model.model.encoder.layer.5.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "105 | model.model.encoder.layer.5.attention.output.dropout    | Dropout           | 0     \n",
      "106 | model.model.encoder.layer.5.intermediate                | BertIntermediate  | 2 M   \n",
      "107 | model.model.encoder.layer.5.intermediate.dense          | Linear            | 2 M   \n",
      "108 | model.model.encoder.layer.5.output                      | BertOutput        | 2 M   \n",
      "109 | model.model.encoder.layer.5.output.dense                | Linear            | 2 M   \n",
      "110 | model.model.encoder.layer.5.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "111 | model.model.encoder.layer.5.output.dropout              | Dropout           | 0     \n",
      "112 | model.model.encoder.layer.6                             | BertLayer         | 7 M   \n",
      "113 | model.model.encoder.layer.6.attention                   | BertAttention     | 2 M   \n",
      "114 | model.model.encoder.layer.6.attention.self              | BertSelfAttention | 1 M   \n",
      "115 | model.model.encoder.layer.6.attention.self.query        | Linear            | 590 K \n",
      "116 | model.model.encoder.layer.6.attention.self.key          | Linear            | 590 K \n",
      "117 | model.model.encoder.layer.6.attention.self.value        | Linear            | 590 K \n",
      "118 | model.model.encoder.layer.6.attention.self.dropout      | Dropout           | 0     \n",
      "119 | model.model.encoder.layer.6.attention.output            | BertSelfOutput    | 592 K \n",
      "120 | model.model.encoder.layer.6.attention.output.dense      | Linear            | 590 K \n",
      "121 | model.model.encoder.layer.6.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "122 | model.model.encoder.layer.6.attention.output.dropout    | Dropout           | 0     \n",
      "123 | model.model.encoder.layer.6.intermediate                | BertIntermediate  | 2 M   \n",
      "124 | model.model.encoder.layer.6.intermediate.dense          | Linear            | 2 M   \n",
      "125 | model.model.encoder.layer.6.output                      | BertOutput        | 2 M   \n",
      "126 | model.model.encoder.layer.6.output.dense                | Linear            | 2 M   \n",
      "127 | model.model.encoder.layer.6.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "128 | model.model.encoder.layer.6.output.dropout              | Dropout           | 0     \n",
      "129 | model.model.encoder.layer.7                             | BertLayer         | 7 M   \n",
      "130 | model.model.encoder.layer.7.attention                   | BertAttention     | 2 M   \n",
      "131 | model.model.encoder.layer.7.attention.self              | BertSelfAttention | 1 M   \n",
      "132 | model.model.encoder.layer.7.attention.self.query        | Linear            | 590 K \n",
      "133 | model.model.encoder.layer.7.attention.self.key          | Linear            | 590 K \n",
      "134 | model.model.encoder.layer.7.attention.self.value        | Linear            | 590 K \n",
      "135 | model.model.encoder.layer.7.attention.self.dropout      | Dropout           | 0     \n",
      "136 | model.model.encoder.layer.7.attention.output            | BertSelfOutput    | 592 K \n",
      "137 | model.model.encoder.layer.7.attention.output.dense      | Linear            | 590 K \n",
      "138 | model.model.encoder.layer.7.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "139 | model.model.encoder.layer.7.attention.output.dropout    | Dropout           | 0     \n",
      "140 | model.model.encoder.layer.7.intermediate                | BertIntermediate  | 2 M   \n",
      "141 | model.model.encoder.layer.7.intermediate.dense          | Linear            | 2 M   \n",
      "142 | model.model.encoder.layer.7.output                      | BertOutput        | 2 M   \n",
      "143 | model.model.encoder.layer.7.output.dense                | Linear            | 2 M   \n",
      "144 | model.model.encoder.layer.7.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "145 | model.model.encoder.layer.7.output.dropout              | Dropout           | 0     \n",
      "146 | model.model.encoder.layer.8                             | BertLayer         | 7 M   \n",
      "147 | model.model.encoder.layer.8.attention                   | BertAttention     | 2 M   \n",
      "148 | model.model.encoder.layer.8.attention.self              | BertSelfAttention | 1 M   \n",
      "149 | model.model.encoder.layer.8.attention.self.query        | Linear            | 590 K \n",
      "150 | model.model.encoder.layer.8.attention.self.key          | Linear            | 590 K \n",
      "151 | model.model.encoder.layer.8.attention.self.value        | Linear            | 590 K \n",
      "152 | model.model.encoder.layer.8.attention.self.dropout      | Dropout           | 0     \n",
      "153 | model.model.encoder.layer.8.attention.output            | BertSelfOutput    | 592 K \n",
      "154 | model.model.encoder.layer.8.attention.output.dense      | Linear            | 590 K \n",
      "155 | model.model.encoder.layer.8.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "156 | model.model.encoder.layer.8.attention.output.dropout    | Dropout           | 0     \n",
      "157 | model.model.encoder.layer.8.intermediate                | BertIntermediate  | 2 M   \n",
      "158 | model.model.encoder.layer.8.intermediate.dense          | Linear            | 2 M   \n",
      "159 | model.model.encoder.layer.8.output                      | BertOutput        | 2 M   \n",
      "160 | model.model.encoder.layer.8.output.dense                | Linear            | 2 M   \n",
      "161 | model.model.encoder.layer.8.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "162 | model.model.encoder.layer.8.output.dropout              | Dropout           | 0     \n",
      "163 | model.model.encoder.layer.9                             | BertLayer         | 7 M   \n",
      "164 | model.model.encoder.layer.9.attention                   | BertAttention     | 2 M   \n",
      "165 | model.model.encoder.layer.9.attention.self              | BertSelfAttention | 1 M   \n",
      "166 | model.model.encoder.layer.9.attention.self.query        | Linear            | 590 K \n",
      "167 | model.model.encoder.layer.9.attention.self.key          | Linear            | 590 K \n",
      "168 | model.model.encoder.layer.9.attention.self.value        | Linear            | 590 K \n",
      "169 | model.model.encoder.layer.9.attention.self.dropout      | Dropout           | 0     \n",
      "170 | model.model.encoder.layer.9.attention.output            | BertSelfOutput    | 592 K \n",
      "171 | model.model.encoder.layer.9.attention.output.dense      | Linear            | 590 K \n",
      "172 | model.model.encoder.layer.9.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "173 | model.model.encoder.layer.9.attention.output.dropout    | Dropout           | 0     \n",
      "174 | model.model.encoder.layer.9.intermediate                | BertIntermediate  | 2 M   \n",
      "175 | model.model.encoder.layer.9.intermediate.dense          | Linear            | 2 M   \n",
      "176 | model.model.encoder.layer.9.output                      | BertOutput        | 2 M   \n",
      "177 | model.model.encoder.layer.9.output.dense                | Linear            | 2 M   \n",
      "178 | model.model.encoder.layer.9.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "179 | model.model.encoder.layer.9.output.dropout              | Dropout           | 0     \n",
      "180 | model.model.encoder.layer.10                            | BertLayer         | 7 M   \n",
      "181 | model.model.encoder.layer.10.attention                  | BertAttention     | 2 M   \n",
      "182 | model.model.encoder.layer.10.attention.self             | BertSelfAttention | 1 M   \n",
      "183 | model.model.encoder.layer.10.attention.self.query       | Linear            | 590 K \n",
      "184 | model.model.encoder.layer.10.attention.self.key         | Linear            | 590 K \n",
      "185 | model.model.encoder.layer.10.attention.self.value       | Linear            | 590 K \n",
      "186 | model.model.encoder.layer.10.attention.self.dropout     | Dropout           | 0     \n",
      "187 | model.model.encoder.layer.10.attention.output           | BertSelfOutput    | 592 K \n",
      "188 | model.model.encoder.layer.10.attention.output.dense     | Linear            | 590 K \n",
      "189 | model.model.encoder.layer.10.attention.output.LayerNorm | LayerNorm         | 1 K   \n",
      "190 | model.model.encoder.layer.10.attention.output.dropout   | Dropout           | 0     \n",
      "191 | model.model.encoder.layer.10.intermediate               | BertIntermediate  | 2 M   \n",
      "192 | model.model.encoder.layer.10.intermediate.dense         | Linear            | 2 M   \n",
      "193 | model.model.encoder.layer.10.output                     | BertOutput        | 2 M   \n",
      "194 | model.model.encoder.layer.10.output.dense               | Linear            | 2 M   \n",
      "195 | model.model.encoder.layer.10.output.LayerNorm           | LayerNorm         | 1 K   \n",
      "196 | model.model.encoder.layer.10.output.dropout             | Dropout           | 0     \n",
      "197 | model.model.encoder.layer.11                            | BertLayer         | 7 M   \n",
      "198 | model.model.encoder.layer.11.attention                  | BertAttention     | 2 M   \n",
      "199 | model.model.encoder.layer.11.attention.self             | BertSelfAttention | 1 M   \n",
      "200 | model.model.encoder.layer.11.attention.self.query       | Linear            | 590 K \n",
      "201 | model.model.encoder.layer.11.attention.self.key         | Linear            | 590 K \n",
      "202 | model.model.encoder.layer.11.attention.self.value       | Linear            | 590 K \n",
      "203 | model.model.encoder.layer.11.attention.self.dropout     | Dropout           | 0     \n",
      "204 | model.model.encoder.layer.11.attention.output           | BertSelfOutput    | 592 K \n",
      "205 | model.model.encoder.layer.11.attention.output.dense     | Linear            | 590 K \n",
      "206 | model.model.encoder.layer.11.attention.output.LayerNorm | LayerNorm         | 1 K   \n",
      "207 | model.model.encoder.layer.11.attention.output.dropout   | Dropout           | 0     \n",
      "208 | model.model.encoder.layer.11.intermediate               | BertIntermediate  | 2 M   \n",
      "209 | model.model.encoder.layer.11.intermediate.dense         | Linear            | 2 M   \n",
      "210 | model.model.encoder.layer.11.output                     | BertOutput        | 2 M   \n",
      "211 | model.model.encoder.layer.11.output.dense               | Linear            | 2 M   \n",
      "212 | model.model.encoder.layer.11.output.LayerNorm           | LayerNorm         | 1 K   \n",
      "213 | model.model.encoder.layer.11.output.dropout             | Dropout           | 0     \n",
      "214 | model.model.pooler                                      | BertPooler        | 590 K \n",
      "215 | model.model.pooler.dense                                | Linear            | 590 K \n",
      "216 | model.model.pooler.activation                           | Tanh              | 0     \n",
      "217 | model.dropout                                           | Dropout           | 0     \n",
      "218 | model.fc1                                               | Linear            | 590 K \n",
      "219 | model.fc2                                               | Linear            | 1 K   \n",
      "220 | model.activation                                        | Mish              | 0     \n",
      "221 | loss                                                    | CrossEntropyLoss  | 0     \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f35cae7186f41548031d7398e64a9e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Validation sanity check', layout=Layout(fle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'valid_loss': tensor(0.6526), 'log': {'valid_loss': tensor(0.6526)}, 'progress_bar': {'valid_loss': tensor(0.6526)}}\n",
      "{'valid_loss': tensor(0.6508), 'log': {'valid_loss': tensor(0.6508)}, 'progress_bar': {'valid_loss': tensor(0.6508)}}\n",
      "{'valid_loss': tensor(0.6414), 'log': {'valid_loss': tensor(0.6414)}, 'progress_bar': {'valid_loss': tensor(0.6414)}}\n",
      "{'valid_loss': tensor(0.6631), 'log': {'valid_loss': tensor(0.6631)}, 'progress_bar': {'valid_loss': tensor(0.6631)}}\n",
      "{'valid_loss': tensor(0.6411), 'log': {'valid_loss': tensor(0.6411)}, 'progress_bar': {'valid_loss': tensor(0.6411)}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f563d220a344dfba741d59f4ec0780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Training', layout=Layout(flex='2'), max=1, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(0.7634, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.7634, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.7634, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.7698, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.7698, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.7698, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.7846, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.7846, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.7846, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.7392, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.7392, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.7392, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.7318, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.7318, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.7318, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.7024, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.7024, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.7024, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.6501, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.6501, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.6501, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.6127, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.6127, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.6127, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.5495, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.5495, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.5495, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.5012, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.5012, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.5012, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.4485, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.4485, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.4485, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.4423, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.4423, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.4423, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.3441, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.3441, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.3441, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.2958, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.2958, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.2958, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.2411, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.2411, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.2411, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.2271, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.2271, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.2271, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.2082, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.2082, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.2082, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.1555, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.1555, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.1555, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.1523, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.1523, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.1523, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.1311, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.1311, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.1311, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0849, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0849, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0849, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0794, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0794, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0794, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0554, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0554, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0554, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0813, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0813, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0813, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0418, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0418, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0418, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0718, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0718, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0718, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0270, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0270, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0270, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0303, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0303, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0303, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0217, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0217, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0217, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0248, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0248, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0248, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0466, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0466, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0466, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0304, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0304, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0304, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0156, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0156, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0156, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0142, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0142, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0142, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0585, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0585, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0585, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0569, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0569, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0569, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0057, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0057, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0057, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0132, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0132, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0132, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0115, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0115, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0115, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0606, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0606, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0606, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0483, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0483, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0483, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0466, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0466, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0466, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0101, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0101, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0101, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0084, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0084, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0084, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0495, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0495, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0495, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0109, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0109, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0109, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0497, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0497, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0497, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0105, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0105, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0105, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0038, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0038, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0038, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0068, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0068, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0068, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0052, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0052, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0052, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0033, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0033, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0033, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0085, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0085, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0085, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0452, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0452, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0452, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0028, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0028, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0028, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0058, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0058, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0058, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0447, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0447, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0447, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0035, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0035, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0035, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0471, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0471, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0471, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0012, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0012, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0012, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0014, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0014, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0014, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0058, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0058, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0058, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0035, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0035, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0035, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0010, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0010, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0010, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0436, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0436, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0436, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0019, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0019, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0019, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0011, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0011, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0011, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0037, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0037, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0037, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0020, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0020, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0020, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0022, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0022, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0022, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0032, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0032, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0032, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0453, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0453, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0453, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0479, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0479, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0479, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0031, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0031, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0031, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0019, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0019, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0019, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0461, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0461, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0461, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0039, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0039, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0039, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0021, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0021, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0021, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0011, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0011, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0011, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0012, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0012, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0012, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0030, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0030, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0030, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0061, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0061, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0061, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0018, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0018, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0018, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0008, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0008, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0008, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0025, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0025, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0025, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0011, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0011, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0011, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0042, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0042, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0042, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0013, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0013, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0013, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0010, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0010, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0010, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0493, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0493, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0493, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0880, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0880, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0880, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0048, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0048, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0048, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0007, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0029, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0029, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0029, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0030, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0030, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0030, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0023, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0023, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0023, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0008, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0008, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0008, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0013, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0013, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0013, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0027, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0027, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0027, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0015, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0015, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0015, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0021, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0021, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0021, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0016, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0016, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0016, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(5.5020e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(5.5020e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(5.5020e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0025, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0025, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0025, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0453, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0453, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0453, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0009, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0009, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0009, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0032, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0032, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0032, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0007, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0011, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0011, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0011, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0443, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0443, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0443, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0445, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0445, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0445, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0007, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0024, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0024, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0024, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0439, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0439, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0439, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0029, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0029, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0029, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0016, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0016, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0016, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(5.2762e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(5.2762e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(5.2762e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(9.2574e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(9.2574e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(9.2574e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0012, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0012, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0012, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0027, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0027, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0027, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0009, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0009, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0009, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0441, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0441, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0441, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0008, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0008, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0008, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0031, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0031, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0031, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0015, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0015, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0015, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0012, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0012, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0012, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0008, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0008, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0008, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0009, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0009, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0009, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0011, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0011, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0011, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0011, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0011, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0011, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0011, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0011, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0011, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0009, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0009, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0009, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0016, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0016, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0016, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0449, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0449, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0449, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0017, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0017, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0017, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0444, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0444, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0444, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0010, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0010, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0010, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0011, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0011, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0011, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0013, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0013, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0013, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0008, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0008, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0008, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0443, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0443, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0443, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0006, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0008, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0008, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0008, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0441, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0441, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0441, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0006, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0010, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0010, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0010, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0015, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0015, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0015, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0010, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0010, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0010, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0449, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0449, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0449, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0009, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0009, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0009, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0011, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0011, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0011, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0011, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0011, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0011, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0007, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0435, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0007, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0015, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0015, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0015, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0014, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0014, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0014, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0016, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0016, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0016, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0440, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0440, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0440, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0011, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0011, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0011, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0436, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0436, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0436, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0014, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0014, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0014, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0013, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0013, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0013, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0019, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0019, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0019, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0439, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0439, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0439, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0010, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0010, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0010, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0012, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0012, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0012, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0011, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0011, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0011, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0433, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0433, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0433, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(2.4579e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(2.4579e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(2.4579e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0006, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0013, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0013, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0013, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0010, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0010, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0010, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0006, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0012, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0012, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0012, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0008, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0008, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0008, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0015, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0015, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0015, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0013, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0013, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0013, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0434, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0007, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0009, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0009, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0009, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0015, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0015, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0015, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0010, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0010, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0010, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0009, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0009, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0009, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(5.9503e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(5.9503e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(5.9503e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0010, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0010, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0010, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0007, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0438, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0438, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0438, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(6.5039e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(6.5039e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(6.5039e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0441, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0441, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0441, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0009, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0009, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0009, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0010, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0010, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0010, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0006, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0007, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0007, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(1.2792e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(1.2792e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(1.2792e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0008, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0008, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0008, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0011, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0011, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0011, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0012, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0012, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0012, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0009, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0009, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0009, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0440, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0440, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0440, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0435, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0011, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0011, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0011, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0010, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0010, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0010, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0439, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0439, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0439, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0435, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(9.1842e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(9.1842e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(9.1842e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0009, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0009, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0009, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(8.0438e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(8.0438e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(8.0438e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0435, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0433, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0433, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0433, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(7.4512e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(7.4512e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(7.4512e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0437, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0437, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0437, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0435, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0436, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0436, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0436, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0007, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(5.7259e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(5.7259e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(5.7259e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0006, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0011, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0011, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0011, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0007, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0006, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0437, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0437, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0437, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0436, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0436, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0436, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0007, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0435, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(3.6359e-06, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(3.6359e-06, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(3.6359e-06, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0007, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0437, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0437, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0437, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(6.1744e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(6.1744e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(6.1744e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0437, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0437, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0437, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(9.4916e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(9.4916e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(9.4916e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0006, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(3.7327e-06, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(3.7327e-06, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(3.7327e-06, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0441, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0441, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0441, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0007, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(8.0929e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(8.0929e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(8.0929e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0436, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0436, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0436, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0008, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0008, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0008, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0006, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0440, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0440, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0440, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(9.4926e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(9.4926e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(9.4926e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(8.9018e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(8.9018e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(8.9018e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(4.1116e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(4.1116e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(4.1116e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(6.2519e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(6.2519e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(6.2519e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0435, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0867, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0867, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0867, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(9.1715e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(9.1715e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(9.1715e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(8.1602e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(8.1602e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(8.1602e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0435, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0440, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0440, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0440, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(8.4451e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(8.4451e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(8.4451e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(5.3377e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(5.3377e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(5.3377e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0434, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0434, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(5.8114e-06, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(5.8114e-06, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(5.8114e-06, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(4.1425e-06, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(4.1425e-06, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(4.1425e-06, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(7.0379e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(7.0379e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(7.0379e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0434, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0007, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(7.8812e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(7.8812e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(7.8812e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0438, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0438, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0438, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0022, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0022, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0022, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(7.9917e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(7.9917e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(7.9917e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(6.8091e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(6.8091e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(6.8091e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(7.2486e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(7.2486e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(7.2486e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(9.7031e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(9.7031e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(9.7031e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0435, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(3.5912e-06, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(3.5912e-06, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(3.5912e-06, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0867, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0867, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0867, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(6.1885e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(6.1885e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(6.1885e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(7.6635e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(7.6635e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(7.6635e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0008, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0008, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0008, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(3.5388e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(3.5388e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(3.5388e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0433, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0433, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0433, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(8.9413e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(8.9413e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(8.9413e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0434, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0437, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0437, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0437, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(1.4156e-06, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(1.4156e-06, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(1.4156e-06, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(9.8329e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(9.8329e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(9.8329e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0435, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(5.4237e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(5.4237e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(5.4237e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(6.4604e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(6.4604e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(6.4604e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0434, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(9.3781e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(9.3781e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(9.3781e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(6.1590e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(6.1590e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(6.1590e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0434, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(1.9669e-06, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(1.9669e-06, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(1.9669e-06, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(7.5132e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(7.5132e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(7.5132e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(7.8566e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(7.8566e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(7.8566e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(4.4845e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(4.4845e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(4.4845e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0437, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0437, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0437, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0434, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(2.8716e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(2.8716e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(2.8716e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(5.8714e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(5.8714e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(5.8714e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0434, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(1.7700e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(1.7700e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(1.7700e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0437, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0437, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0437, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0437, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0437, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0437, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(4.5894e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(4.5894e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(4.5894e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(6.7566e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(6.7566e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(6.7566e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0435, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0435, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0436, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0436, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0436, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(2.8120e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(2.8120e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(2.8120e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0434, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0434, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(3.2715e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(3.2715e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(3.2715e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(9.1750e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(9.1750e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(9.1750e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(4.9282e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(4.9282e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(4.9282e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(1.0251e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(1.0251e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(1.0251e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(9.3877e-07, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(9.3877e-07, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(9.3877e-07, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0434, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(3.2010e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(3.2010e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(3.2010e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0434, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(6.3273e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(6.3273e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(6.3273e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0434, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0434, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0437, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0437, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0437, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(4.5049e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(4.5049e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(4.5049e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0867, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0867, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0867, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(1.4512e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(1.4512e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(1.4512e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0435, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(7.5200e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(7.5200e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(7.5200e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(3.4107e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(3.4107e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(3.4107e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(5.3082e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(5.3082e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(5.3082e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(9.3740e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(9.3740e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(9.3740e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(5.2695e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(5.2695e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(5.2695e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0435, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(8.8169e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(8.8169e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(8.8169e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(7.2290e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(7.2290e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(7.2290e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(1.6873e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(1.6873e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(1.6873e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0434, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0435, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0436, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0436, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0436, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(7.3673e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(7.3673e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(7.3673e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(4.8340e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(4.8340e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(4.8340e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0434, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(8.5355e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(8.5355e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(8.5355e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(7.5273e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(7.5273e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(7.5273e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(6.2061e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(6.2061e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(6.2061e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0435, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(9.0337e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(9.0337e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(9.0337e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(9.0202e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(9.0202e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(9.0202e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0867, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0867, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0867, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0436, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0436, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0436, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(6.2514e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(6.2514e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(6.2514e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(8.6211e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(8.6211e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(8.6211e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(7.0035e-07, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(7.0035e-07, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(7.0035e-07, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(6.2373e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(6.2373e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(6.2373e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0434, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(9.4560e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(9.4560e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(9.4560e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0434, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0434, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(4.6064e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(4.6064e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(4.6064e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(1.6479e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(1.6479e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(1.6479e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0436, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0436, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0436, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(5.4437e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(5.4437e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(5.4437e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(8.2258e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(8.2258e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(8.2258e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(2.9282e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(2.9282e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(2.9282e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(9.8805e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(9.8805e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(9.8805e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0867, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0867, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0867, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(6.5565e-07, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(6.5565e-07, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(6.5565e-07, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(5.1153e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(5.1153e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(5.1153e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(3.4665e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(3.4665e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(3.4665e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(7.9485e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(7.9485e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(7.9485e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(7.0035e-07, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(7.0035e-07, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(7.0035e-07, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(9.7240e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(9.7240e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(9.7240e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(6.2207e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(6.2207e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(6.2207e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(3.4201e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(3.4201e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(3.4201e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0434, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(3.9488e-07, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(3.9488e-07, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(3.9488e-07, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(3.6423e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(3.6423e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(3.6423e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0437, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0437, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0437, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(7.6949e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(7.6949e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(7.6949e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0433, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0433, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0433, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(1.7737e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(1.7737e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(1.7737e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0434, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(9.0723e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(9.0723e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(9.0723e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(6.3324e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(6.3324e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(6.3324e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(2.1909e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(2.1909e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(2.1909e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0433, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0433, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0433, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(3.5999e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(3.5999e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(3.5999e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0434, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(7.9794e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(7.9794e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(7.9794e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(9.4170e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(9.4170e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(9.4170e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(7.6268e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(7.6268e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(7.6268e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(8.2294e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(8.2294e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(8.2294e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(8.6499e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(8.6499e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(8.6499e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(6.4981e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(6.4981e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(6.4981e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0434, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(2.2894e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(2.2894e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(2.2894e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(3.5819e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(3.5819e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(3.5819e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(3.8968e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(3.8968e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(3.8968e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(8.0701e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(8.0701e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(8.0701e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(9.2223e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(9.2223e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(9.2223e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(8.3160e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(8.3160e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(8.3160e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0433, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0433, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0433, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(7.0533e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(7.0533e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(7.0533e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0433, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0433, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0433, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0433, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0433, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0433, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0434, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0434, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(7.7155e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(7.7155e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(7.7155e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(9.7341e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(9.7341e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(9.7341e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0435, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(5.4758, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(5.4758, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(5.4758, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(13.2592, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(13.2592, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(13.2592, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(9.8803, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(9.8803, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(9.8803, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(8.4127, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(8.4127, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(8.4127, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(6.6380, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(6.6380, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(6.6380, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(4.8279, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(4.8279, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(4.8279, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(3.0376, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(3.0376, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(3.0376, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(1.2657, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(1.2657, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(1.2657, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.4647, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.4647, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.4647, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0749, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0749, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0749, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0865, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0865, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0865, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0193, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0193, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0193, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0138, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0138, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0138, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0151, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0151, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0151, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0129, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0129, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0129, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0102, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0102, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0102, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0066, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0066, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0066, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0013, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0013, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0013, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0007, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0008, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0008, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0008, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0009, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0009, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0009, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0435, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0437, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0437, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0437, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0437, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0437, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0437, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0453, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0453, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0453, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0030, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0030, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0030, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0020, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0020, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0020, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0008, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0008, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0008, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0008, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0008, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0008, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(7.1165e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(7.1165e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(7.1165e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(9.2892e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(9.2892e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(9.2892e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0007, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(4.9597e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(4.9597e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(4.9597e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0010, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0010, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0010, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0009, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0009, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0009, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0435, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(6.4320e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(6.4320e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(6.4320e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(5.8010e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(5.8010e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(5.8010e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0011, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0011, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0011, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0436, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0436, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0436, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(3.7833e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(3.7833e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(3.7833e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0440, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0440, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0440, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0434, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0438, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0438, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0438, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0006, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0006, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0434, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0006, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0437, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0437, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0437, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0434, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0010, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0010, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0010, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0433, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0433, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0433, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0869, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0869, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0869, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0007, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(1.4603e-06, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(1.4603e-06, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(1.4603e-06, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0435, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0439, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0439, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0439, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0009, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0009, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0009, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0441, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0441, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0441, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0007, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0006, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(2.0415e-06, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(2.0415e-06, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(2.0415e-06, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0437, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0437, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0437, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0438, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0438, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0438, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0007, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0010, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0010, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0010, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0445, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0445, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0445, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(9.1823e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(9.1823e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(9.1823e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0007, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0439, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0439, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0439, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0006, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(7.6235e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(7.6235e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(7.6235e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0017, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0017, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0017, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0006, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0434, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0440, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0440, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0440, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(1.2144e-06, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(1.2144e-06, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(1.2144e-06, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0008, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0008, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0008, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(2.8089e-06, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(2.8089e-06, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(2.8089e-06, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(8.1358e-06, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(8.1358e-06, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(8.1358e-06, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(8.1844e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(8.1844e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(8.1844e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0439, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0439, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0439, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0006, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0011, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0011, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0011, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0007, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0006, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0435, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0437, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0437, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0437, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0436, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0436, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0436, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0006, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0006, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(4.8157e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(4.8157e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(4.8157e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0439, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0439, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0439, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0007, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(7.1673e-06, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(7.1673e-06, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(7.1673e-06, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0438, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0438, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0438, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0006, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0434, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0434, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0006, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0008, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0008, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0008, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(4.9788e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(4.9788e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(4.9788e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0007, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0039, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0039, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0039, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0008, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0008, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0008, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0438, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0438, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0438, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0437, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0437, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0437, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0435, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(3.9391e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(3.9391e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(3.9391e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(6.4268e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(6.4268e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(6.4268e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0436, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0436, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0436, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0007, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0434, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0435, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0009, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0009, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0009, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(5.7843e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(5.7843e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(5.7843e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(6.1923e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(6.1923e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(6.1923e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0435, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0007, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0007, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0007, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0006, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0006, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0006, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0434, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0434, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(2.4140e-06, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(2.4140e-06, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(2.4140e-06, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0435, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0435, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0433, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0433, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0433, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(4.1585e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(4.1585e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(4.1585e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(9.1314e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(9.1314e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(9.1314e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(1.6174e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(1.6174e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(1.6174e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(3.8968e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(3.8968e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(3.8968e-05, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0004, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0004, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0005, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0005, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0002, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0002, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0433, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0433, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0433, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0437, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0437, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0437, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0003, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0003, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(2.7045e-06, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(2.7045e-06, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(2.7045e-06, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(2.1681e-06, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(2.1681e-06, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(2.1681e-06, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(0.0001, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(0.0001, grad_fn=<NllLossBackward>)}}\n",
      "{'loss': tensor(8.7976e-05, grad_fn=<NllLossBackward>), 'log': {'train_loss': tensor(8.7976e-05, grad_fn=<NllLossBackward>)}, 'progress_bar': {'train_loss': tensor(8.7976e-05, grad_fn=<NllLossBackward>)}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8faa3c510aa4568a7e5e063e5771620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Validating', layout=Layout(flex='2'), max=1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'valid_loss': tensor(5.6847e-06), 'log': {'valid_loss': tensor(5.6847e-06)}, 'progress_bar': {'valid_loss': tensor(5.6847e-06)}}\n",
      "{'valid_loss': tensor(6.7352e-06), 'log': {'valid_loss': tensor(6.7352e-06)}, 'progress_bar': {'valid_loss': tensor(6.7352e-06)}}\n",
      "{'valid_loss': tensor(3.1963e-06), 'log': {'valid_loss': tensor(3.1963e-06)}, 'progress_bar': {'valid_loss': tensor(3.1963e-06)}}\n",
      "{'valid_loss': tensor(2.9877e-06), 'log': {'valid_loss': tensor(2.9877e-06)}, 'progress_bar': {'valid_loss': tensor(2.9877e-06)}}\n",
      "{'valid_loss': tensor(8.2476e-06), 'log': {'valid_loss': tensor(8.2476e-06)}, 'progress_bar': {'valid_loss': tensor(8.2476e-06)}}\n",
      "{'valid_loss': tensor(2.6296e-05), 'log': {'valid_loss': tensor(2.6296e-05)}, 'progress_bar': {'valid_loss': tensor(2.6296e-05)}}\n",
      "{'valid_loss': tensor(4.6938e-06), 'log': {'valid_loss': tensor(4.6938e-06)}, 'progress_bar': {'valid_loss': tensor(4.6938e-06)}}\n",
      "{'valid_loss': tensor(7.4282e-06), 'log': {'valid_loss': tensor(7.4282e-06)}, 'progress_bar': {'valid_loss': tensor(7.4282e-06)}}\n",
      "{'valid_loss': tensor(4.1053e-06), 'log': {'valid_loss': tensor(4.1053e-06)}, 'progress_bar': {'valid_loss': tensor(4.1053e-06)}}\n",
      "{'valid_loss': tensor(4.6640e-06), 'log': {'valid_loss': tensor(4.6640e-06)}, 'progress_bar': {'valid_loss': tensor(4.6640e-06)}}\n",
      "{'valid_loss': tensor(4.0307e-06), 'log': {'valid_loss': tensor(4.0307e-06)}, 'progress_bar': {'valid_loss': tensor(4.0307e-06)}}\n",
      "{'valid_loss': tensor(7.4878e-06), 'log': {'valid_loss': tensor(7.4878e-06)}, 'progress_bar': {'valid_loss': tensor(7.4878e-06)}}\n",
      "{'valid_loss': tensor(2.4810e-06), 'log': {'valid_loss': tensor(2.4810e-06)}, 'progress_bar': {'valid_loss': tensor(2.4810e-06)}}\n",
      "{'valid_loss': tensor(6.3702e-06), 'log': {'valid_loss': tensor(6.3702e-06)}, 'progress_bar': {'valid_loss': tensor(6.3702e-06)}}\n",
      "{'valid_loss': tensor(2.4363e-06), 'log': {'valid_loss': tensor(2.4363e-06)}, 'progress_bar': {'valid_loss': tensor(2.4363e-06)}}\n",
      "{'valid_loss': tensor(4.5597e-06), 'log': {'valid_loss': tensor(4.5597e-06)}, 'progress_bar': {'valid_loss': tensor(4.5597e-06)}}\n",
      "{'valid_loss': tensor(2.3544e-06), 'log': {'valid_loss': tensor(2.3544e-06)}, 'progress_bar': {'valid_loss': tensor(2.3544e-06)}}\n",
      "{'valid_loss': tensor(5.0514e-06), 'log': {'valid_loss': tensor(5.0514e-06)}, 'progress_bar': {'valid_loss': tensor(5.0514e-06)}}\n",
      "{'valid_loss': tensor(3.7402e-06), 'log': {'valid_loss': tensor(3.7402e-06)}, 'progress_bar': {'valid_loss': tensor(3.7402e-06)}}\n",
      "{'valid_loss': tensor(1.2874e-05), 'log': {'valid_loss': tensor(1.2874e-05)}, 'progress_bar': {'valid_loss': tensor(1.2874e-05)}}\n",
      "{'valid_loss': tensor(1.0229e-05), 'log': {'valid_loss': tensor(1.0229e-05)}, 'progress_bar': {'valid_loss': tensor(1.0229e-05)}}\n",
      "{'valid_loss': tensor(3.3677e-06), 'log': {'valid_loss': tensor(3.3677e-06)}, 'progress_bar': {'valid_loss': tensor(3.3677e-06)}}\n",
      "{'valid_loss': tensor(1.4215e-05), 'log': {'valid_loss': tensor(1.4215e-05)}, 'progress_bar': {'valid_loss': tensor(1.4215e-05)}}\n",
      "{'valid_loss': tensor(2.6300e-06), 'log': {'valid_loss': tensor(2.6300e-06)}, 'progress_bar': {'valid_loss': tensor(2.6300e-06)}}\n",
      "{'valid_loss': tensor(4.5225e-06), 'log': {'valid_loss': tensor(4.5225e-06)}, 'progress_bar': {'valid_loss': tensor(4.5225e-06)}}\n",
      "{'valid_loss': tensor(7.8826e-06), 'log': {'valid_loss': tensor(7.8826e-06)}, 'progress_bar': {'valid_loss': tensor(7.8826e-06)}}\n",
      "{'valid_loss': tensor(1.1101e-05), 'log': {'valid_loss': tensor(1.1101e-05)}, 'progress_bar': {'valid_loss': tensor(1.1101e-05)}}\n",
      "{'valid_loss': tensor(4.9025e-06), 'log': {'valid_loss': tensor(4.9025e-06)}, 'progress_bar': {'valid_loss': tensor(4.9025e-06)}}\n",
      "{'valid_loss': tensor(5.1036e-06), 'log': {'valid_loss': tensor(5.1036e-06)}, 'progress_bar': {'valid_loss': tensor(5.1036e-06)}}\n",
      "{'valid_loss': tensor(3.8147e-06), 'log': {'valid_loss': tensor(3.8147e-06)}, 'progress_bar': {'valid_loss': tensor(3.8147e-06)}}\n",
      "{'valid_loss': tensor(4.6044e-06), 'log': {'valid_loss': tensor(4.6044e-06)}, 'progress_bar': {'valid_loss': tensor(4.6044e-06)}}\n",
      "{'valid_loss': tensor(7.8454e-06), 'log': {'valid_loss': tensor(7.8454e-06)}, 'progress_bar': {'valid_loss': tensor(7.8454e-06)}}\n",
      "{'valid_loss': tensor(6.3478e-06), 'log': {'valid_loss': tensor(6.3478e-06)}, 'progress_bar': {'valid_loss': tensor(6.3478e-06)}}\n",
      "{'valid_loss': tensor(6.2957e-06), 'log': {'valid_loss': tensor(6.2957e-06)}, 'progress_bar': {'valid_loss': tensor(6.2957e-06)}}\n",
      "{'valid_loss': tensor(3.3900e-06), 'log': {'valid_loss': tensor(3.3900e-06)}, 'progress_bar': {'valid_loss': tensor(3.3900e-06)}}\n",
      "{'valid_loss': tensor(5.6401e-06), 'log': {'valid_loss': tensor(5.6401e-06)}, 'progress_bar': {'valid_loss': tensor(5.6401e-06)}}\n",
      "{'valid_loss': tensor(5.1483e-06), 'log': {'valid_loss': tensor(5.1483e-06)}, 'progress_bar': {'valid_loss': tensor(5.1483e-06)}}\n",
      "{'valid_loss': tensor(8.5605e-06), 'log': {'valid_loss': tensor(8.5605e-06)}, 'progress_bar': {'valid_loss': tensor(8.5605e-06)}}\n",
      "{'valid_loss': tensor(6.6608e-06), 'log': {'valid_loss': tensor(6.6608e-06)}, 'progress_bar': {'valid_loss': tensor(6.6608e-06)}}\n",
      "{'valid_loss': tensor(1.5825e-05), 'log': {'valid_loss': tensor(1.5825e-05)}, 'progress_bar': {'valid_loss': tensor(1.5825e-05)}}\n",
      "{'valid_loss': tensor(3.3975e-06), 'log': {'valid_loss': tensor(3.3975e-06)}, 'progress_bar': {'valid_loss': tensor(3.3975e-06)}}\n",
      "{'valid_loss': tensor(4.4927e-06), 'log': {'valid_loss': tensor(4.4927e-06)}, 'progress_bar': {'valid_loss': tensor(4.4927e-06)}}\n",
      "{'valid_loss': tensor(2.6226e-06), 'log': {'valid_loss': tensor(2.6226e-06)}, 'progress_bar': {'valid_loss': tensor(2.6226e-06)}}\n",
      "{'valid_loss': tensor(1.0833e-05), 'log': {'valid_loss': tensor(1.0833e-05)}, 'progress_bar': {'valid_loss': tensor(1.0833e-05)}}\n",
      "{'valid_loss': tensor(5.8040e-06), 'log': {'valid_loss': tensor(5.8040e-06)}, 'progress_bar': {'valid_loss': tensor(5.8040e-06)}}\n",
      "{'valid_loss': tensor(7.3536e-06), 'log': {'valid_loss': tensor(7.3536e-06)}, 'progress_bar': {'valid_loss': tensor(7.3536e-06)}}\n",
      "{'valid_loss': tensor(4.5821e-06), 'log': {'valid_loss': tensor(4.5821e-06)}, 'progress_bar': {'valid_loss': tensor(4.5821e-06)}}\n",
      "{'valid_loss': tensor(4.1723e-06), 'log': {'valid_loss': tensor(4.1723e-06)}, 'progress_bar': {'valid_loss': tensor(4.1723e-06)}}\n",
      "{'valid_loss': tensor(5.2824e-06), 'log': {'valid_loss': tensor(5.2824e-06)}, 'progress_bar': {'valid_loss': tensor(5.2824e-06)}}\n",
      "{'valid_loss': tensor(2.8014e-06), 'log': {'valid_loss': tensor(2.8014e-06)}, 'progress_bar': {'valid_loss': tensor(2.8014e-06)}}\n",
      "{'valid_loss': tensor(9.9985e-06), 'log': {'valid_loss': tensor(9.9985e-06)}, 'progress_bar': {'valid_loss': tensor(9.9985e-06)}}\n",
      "{'valid_loss': tensor(6.7204e-06), 'log': {'valid_loss': tensor(6.7204e-06)}, 'progress_bar': {'valid_loss': tensor(6.7204e-06)}}\n",
      "{'valid_loss': tensor(4.7236e-06), 'log': {'valid_loss': tensor(4.7236e-06)}, 'progress_bar': {'valid_loss': tensor(4.7236e-06)}}\n",
      "{'valid_loss': tensor(4.4480e-06), 'log': {'valid_loss': tensor(4.4480e-06)}, 'progress_bar': {'valid_loss': tensor(4.4480e-06)}}\n",
      "{'valid_loss': tensor(1.1384e-05), 'log': {'valid_loss': tensor(1.1384e-05)}, 'progress_bar': {'valid_loss': tensor(1.1384e-05)}}\n",
      "{'valid_loss': tensor(7.2270e-06), 'log': {'valid_loss': tensor(7.2270e-06)}, 'progress_bar': {'valid_loss': tensor(7.2270e-06)}}\n",
      "{'valid_loss': tensor(8.8586e-06), 'log': {'valid_loss': tensor(8.8586e-06)}, 'progress_bar': {'valid_loss': tensor(8.8586e-06)}}\n",
      "{'valid_loss': tensor(1.1056e-05), 'log': {'valid_loss': tensor(1.1056e-05)}, 'progress_bar': {'valid_loss': tensor(1.1056e-05)}}\n",
      "{'valid_loss': tensor(3.0324e-06), 'log': {'valid_loss': tensor(3.0324e-06)}, 'progress_bar': {'valid_loss': tensor(3.0324e-06)}}\n",
      "{'valid_loss': tensor(3.2410e-06), 'log': {'valid_loss': tensor(3.2410e-06)}, 'progress_bar': {'valid_loss': tensor(3.2410e-06)}}\n",
      "{'valid_loss': tensor(1.1950e-05), 'log': {'valid_loss': tensor(1.1950e-05)}, 'progress_bar': {'valid_loss': tensor(1.1950e-05)}}\n",
      "{'valid_loss': tensor(6.4075e-06), 'log': {'valid_loss': tensor(6.4075e-06)}, 'progress_bar': {'valid_loss': tensor(6.4075e-06)}}\n",
      "{'valid_loss': tensor(2.8834e-06), 'log': {'valid_loss': tensor(2.8834e-06)}, 'progress_bar': {'valid_loss': tensor(2.8834e-06)}}\n",
      "{'valid_loss': tensor(4.0606e-06), 'log': {'valid_loss': tensor(4.0606e-06)}, 'progress_bar': {'valid_loss': tensor(4.0606e-06)}}\n",
      "{'valid_loss': tensor(2.8759e-06), 'log': {'valid_loss': tensor(2.8759e-06)}, 'progress_bar': {'valid_loss': tensor(2.8759e-06)}}\n",
      "{'valid_loss': tensor(3.3229e-06), 'log': {'valid_loss': tensor(3.3229e-06)}, 'progress_bar': {'valid_loss': tensor(3.3229e-06)}}\n",
      "{'valid_loss': tensor(3.1814e-06), 'log': {'valid_loss': tensor(3.1814e-06)}, 'progress_bar': {'valid_loss': tensor(3.1814e-06)}}\n",
      "{'valid_loss': tensor(6.9513e-06), 'log': {'valid_loss': tensor(6.9513e-06)}, 'progress_bar': {'valid_loss': tensor(6.9513e-06)}}\n",
      "{'valid_loss': tensor(2.7791e-06), 'log': {'valid_loss': tensor(2.7791e-06)}, 'progress_bar': {'valid_loss': tensor(2.7791e-06)}}\n",
      "{'valid_loss': tensor(4.8428e-06), 'log': {'valid_loss': tensor(4.8428e-06)}, 'progress_bar': {'valid_loss': tensor(4.8428e-06)}}\n",
      "{'valid_loss': tensor(4.4182e-06), 'log': {'valid_loss': tensor(4.4182e-06)}, 'progress_bar': {'valid_loss': tensor(4.4182e-06)}}\n",
      "{'valid_loss': tensor(9.8940e-05), 'log': {'valid_loss': tensor(9.8940e-05)}, 'progress_bar': {'valid_loss': tensor(9.8940e-05)}}\n",
      "{'valid_loss': tensor(6.4521e-06), 'log': {'valid_loss': tensor(6.4521e-06)}, 'progress_bar': {'valid_loss': tensor(6.4521e-06)}}\n",
      "{'valid_loss': tensor(1.7061e-05), 'log': {'valid_loss': tensor(1.7061e-05)}, 'progress_bar': {'valid_loss': tensor(1.7061e-05)}}\n",
      "{'valid_loss': tensor(3.5465e-06), 'log': {'valid_loss': tensor(3.5465e-06)}, 'progress_bar': {'valid_loss': tensor(3.5465e-06)}}\n",
      "{'valid_loss': tensor(1.4141e-05), 'log': {'valid_loss': tensor(1.4141e-05)}, 'progress_bar': {'valid_loss': tensor(1.4141e-05)}}\n",
      "{'valid_loss': tensor(9.9687e-06), 'log': {'valid_loss': tensor(9.9687e-06)}, 'progress_bar': {'valid_loss': tensor(9.9687e-06)}}\n",
      "{'valid_loss': tensor(5.4612e-06), 'log': {'valid_loss': tensor(5.4612e-06)}, 'progress_bar': {'valid_loss': tensor(5.4612e-06)}}\n",
      "{'valid_loss': tensor(3.7402e-06), 'log': {'valid_loss': tensor(3.7402e-06)}, 'progress_bar': {'valid_loss': tensor(3.7402e-06)}}\n",
      "{'valid_loss': tensor(5.6251e-06), 'log': {'valid_loss': tensor(5.6251e-06)}, 'progress_bar': {'valid_loss': tensor(5.6251e-06)}}\n",
      "{'valid_loss': tensor(6.0647e-06), 'log': {'valid_loss': tensor(6.0647e-06)}, 'progress_bar': {'valid_loss': tensor(6.0647e-06)}}\n",
      "{'valid_loss': tensor(2.3207e-05), 'log': {'valid_loss': tensor(2.3207e-05)}, 'progress_bar': {'valid_loss': tensor(2.3207e-05)}}\n",
      "{'valid_loss': tensor(2.1011e-06), 'log': {'valid_loss': tensor(2.1011e-06)}, 'progress_bar': {'valid_loss': tensor(2.1011e-06)}}\n",
      "{'valid_loss': tensor(3.0696e-06), 'log': {'valid_loss': tensor(3.0696e-06)}, 'progress_bar': {'valid_loss': tensor(3.0696e-06)}}\n",
      "{'valid_loss': tensor(4.2915e-06), 'log': {'valid_loss': tensor(4.2915e-06)}, 'progress_bar': {'valid_loss': tensor(4.2915e-06)}}\n",
      "{'valid_loss': tensor(2.9802e-06), 'log': {'valid_loss': tensor(2.9802e-06)}, 'progress_bar': {'valid_loss': tensor(2.9802e-06)}}\n",
      "{'valid_loss': tensor(4.0605e-06), 'log': {'valid_loss': tensor(4.0605e-06)}, 'progress_bar': {'valid_loss': tensor(4.0605e-06)}}\n",
      "{'valid_loss': tensor(5.7443e-06), 'log': {'valid_loss': tensor(5.7443e-06)}, 'progress_bar': {'valid_loss': tensor(5.7443e-06)}}\n",
      "{'valid_loss': tensor(3.0249e-06), 'log': {'valid_loss': tensor(3.0249e-06)}, 'progress_bar': {'valid_loss': tensor(3.0249e-06)}}\n",
      "{'valid_loss': tensor(3.8892e-06), 'log': {'valid_loss': tensor(3.8892e-06)}, 'progress_bar': {'valid_loss': tensor(3.8892e-06)}}\n",
      "{'valid_loss': tensor(5.9008e-06), 'log': {'valid_loss': tensor(5.9008e-06)}, 'progress_bar': {'valid_loss': tensor(5.9008e-06)}}\n",
      "{'valid_loss': tensor(5.5879e-06), 'log': {'valid_loss': tensor(5.5879e-06)}, 'progress_bar': {'valid_loss': tensor(5.5879e-06)}}\n",
      "{'valid_loss': tensor(8.2700e-06), 'log': {'valid_loss': tensor(8.2700e-06)}, 'progress_bar': {'valid_loss': tensor(8.2700e-06)}}\n",
      "{'valid_loss': tensor(8.5084e-06), 'log': {'valid_loss': tensor(8.5084e-06)}, 'progress_bar': {'valid_loss': tensor(8.5084e-06)}}\n",
      "{'valid_loss': tensor(1.3656e-05), 'log': {'valid_loss': tensor(1.3656e-05)}, 'progress_bar': {'valid_loss': tensor(1.3656e-05)}}\n",
      "{'valid_loss': tensor(3.6806e-06), 'log': {'valid_loss': tensor(3.6806e-06)}, 'progress_bar': {'valid_loss': tensor(3.6806e-06)}}\n",
      "{'valid_loss': tensor(3.5837e-06), 'log': {'valid_loss': tensor(3.5837e-06)}, 'progress_bar': {'valid_loss': tensor(3.5837e-06)}}\n",
      "{'valid_loss': tensor(1.5593e-05), 'log': {'valid_loss': tensor(1.5593e-05)}, 'progress_bar': {'valid_loss': tensor(1.5593e-05)}}\n",
      "{'valid_loss': tensor(4.3213e-06), 'log': {'valid_loss': tensor(4.3213e-06)}, 'progress_bar': {'valid_loss': tensor(4.3213e-06)}}\n",
      "{'valid_loss': tensor(1.4543e-05), 'log': {'valid_loss': tensor(1.4543e-05)}, 'progress_bar': {'valid_loss': tensor(1.4543e-05)}}\n",
      "{'valid_loss': tensor(3.4794e-06), 'log': {'valid_loss': tensor(3.4794e-06)}, 'progress_bar': {'valid_loss': tensor(3.4794e-06)}}\n",
      "{'valid_loss': tensor(4.7981e-06), 'log': {'valid_loss': tensor(4.7981e-06)}, 'progress_bar': {'valid_loss': tensor(4.7981e-06)}}\n",
      "{'valid_loss': tensor(8.2328e-06), 'log': {'valid_loss': tensor(8.2328e-06)}, 'progress_bar': {'valid_loss': tensor(8.2328e-06)}}\n",
      "{'valid_loss': tensor(2.5555e-06), 'log': {'valid_loss': tensor(2.5555e-06)}, 'progress_bar': {'valid_loss': tensor(2.5555e-06)}}\n",
      "{'valid_loss': tensor(9.5887e-06), 'log': {'valid_loss': tensor(9.5887e-06)}, 'progress_bar': {'valid_loss': tensor(9.5887e-06)}}\n",
      "{'valid_loss': tensor(2.6151e-06), 'log': {'valid_loss': tensor(2.6151e-06)}, 'progress_bar': {'valid_loss': tensor(2.6151e-06)}}\n",
      "{'valid_loss': tensor(2.3325e-05), 'log': {'valid_loss': tensor(2.3325e-05)}, 'progress_bar': {'valid_loss': tensor(2.3325e-05)}}\n",
      "{'valid_loss': tensor(4.6044e-06), 'log': {'valid_loss': tensor(4.6044e-06)}, 'progress_bar': {'valid_loss': tensor(4.6044e-06)}}\n",
      "{'valid_loss': tensor(5.4687e-06), 'log': {'valid_loss': tensor(5.4687e-06)}, 'progress_bar': {'valid_loss': tensor(5.4687e-06)}}\n",
      "{'valid_loss': tensor(7.5101e-06), 'log': {'valid_loss': tensor(7.5101e-06)}, 'progress_bar': {'valid_loss': tensor(7.5101e-06)}}\n",
      "{'valid_loss': tensor(1.1228e-05), 'log': {'valid_loss': tensor(1.1228e-05)}, 'progress_bar': {'valid_loss': tensor(1.1228e-05)}}\n",
      "{'valid_loss': tensor(1.1861e-05), 'log': {'valid_loss': tensor(1.1861e-05)}, 'progress_bar': {'valid_loss': tensor(1.1861e-05)}}\n",
      "{'valid_loss': tensor(2.7567e-06), 'log': {'valid_loss': tensor(2.7567e-06)}, 'progress_bar': {'valid_loss': tensor(2.7567e-06)}}\n",
      "{'valid_loss': tensor(3.1069e-06), 'log': {'valid_loss': tensor(3.1069e-06)}, 'progress_bar': {'valid_loss': tensor(3.1069e-06)}}\n",
      "{'valid_loss': tensor(1.2956e-05), 'log': {'valid_loss': tensor(1.2956e-05)}, 'progress_bar': {'valid_loss': tensor(1.2956e-05)}}\n",
      "{'valid_loss': tensor(1.0744e-05), 'log': {'valid_loss': tensor(1.0744e-05)}, 'progress_bar': {'valid_loss': tensor(1.0744e-05)}}\n",
      "{'valid_loss': tensor(1.0177e-05), 'log': {'valid_loss': tensor(1.0177e-05)}, 'progress_bar': {'valid_loss': tensor(1.0177e-05)}}\n",
      "{'valid_loss': tensor(5.1334e-06), 'log': {'valid_loss': tensor(5.1334e-06)}, 'progress_bar': {'valid_loss': tensor(5.1334e-06)}}\n",
      "{'valid_loss': tensor(5.0887e-06), 'log': {'valid_loss': tensor(5.0887e-06)}, 'progress_bar': {'valid_loss': tensor(5.0887e-06)}}\n",
      "{'valid_loss': tensor(4.6566e-06), 'log': {'valid_loss': tensor(4.6566e-06)}, 'progress_bar': {'valid_loss': tensor(4.6566e-06)}}\n",
      "{'valid_loss': tensor(2.4587e-06), 'log': {'valid_loss': tensor(2.4587e-06)}, 'progress_bar': {'valid_loss': tensor(2.4587e-06)}}\n",
      "{'valid_loss': tensor(2.6431e-05), 'log': {'valid_loss': tensor(2.6431e-05)}, 'progress_bar': {'valid_loss': tensor(2.6431e-05)}}\n",
      "{'valid_loss': tensor(1.1094e-05), 'log': {'valid_loss': tensor(1.1094e-05)}, 'progress_bar': {'valid_loss': tensor(1.1094e-05)}}\n",
      "{'valid_loss': tensor(3.4049e-06), 'log': {'valid_loss': tensor(3.4049e-06)}, 'progress_bar': {'valid_loss': tensor(3.4049e-06)}}\n",
      "{'valid_loss': tensor(8.3818e-06), 'log': {'valid_loss': tensor(8.3818e-06)}, 'progress_bar': {'valid_loss': tensor(8.3818e-06)}}\n",
      "{'valid_loss': tensor(5.0813e-06), 'log': {'valid_loss': tensor(5.0813e-06)}, 'progress_bar': {'valid_loss': tensor(5.0813e-06)}}\n",
      "{'valid_loss': tensor(5.0366e-06), 'log': {'valid_loss': tensor(5.0366e-06)}, 'progress_bar': {'valid_loss': tensor(5.0366e-06)}}\n",
      "{'valid_loss': tensor(5.8189e-06), 'log': {'valid_loss': tensor(5.8189e-06)}, 'progress_bar': {'valid_loss': tensor(5.8189e-06)}}\n",
      "{'valid_loss': tensor(8.7245e-06), 'log': {'valid_loss': tensor(8.7245e-06)}, 'progress_bar': {'valid_loss': tensor(8.7245e-06)}}\n",
      "{'valid_loss': tensor(3.0082e-05), 'log': {'valid_loss': tensor(3.0082e-05)}, 'progress_bar': {'valid_loss': tensor(3.0082e-05)}}\n",
      "{'valid_loss': tensor(4.3660e-06), 'log': {'valid_loss': tensor(4.3660e-06)}, 'progress_bar': {'valid_loss': tensor(4.3660e-06)}}\n",
      "{'valid_loss': tensor(3.1292e-06), 'log': {'valid_loss': tensor(3.1292e-06)}, 'progress_bar': {'valid_loss': tensor(3.1292e-06)}}\n",
      "{'valid_loss': tensor(3.5763e-06), 'log': {'valid_loss': tensor(3.5763e-06)}, 'progress_bar': {'valid_loss': tensor(3.5763e-06)}}\n",
      "{'valid_loss': tensor(3.0771e-06), 'log': {'valid_loss': tensor(3.0771e-06)}, 'progress_bar': {'valid_loss': tensor(3.0771e-06)}}\n",
      "{'valid_loss': tensor(4.9025e-06), 'log': {'valid_loss': tensor(4.9025e-06)}, 'progress_bar': {'valid_loss': tensor(4.9025e-06)}}\n",
      "{'valid_loss': tensor(5.3048e-06), 'log': {'valid_loss': tensor(5.3048e-06)}, 'progress_bar': {'valid_loss': tensor(5.3048e-06)}}\n",
      "{'valid_loss': tensor(4.0605e-06), 'log': {'valid_loss': tensor(4.0605e-06)}, 'progress_bar': {'valid_loss': tensor(4.0605e-06)}}\n",
      "{'valid_loss': tensor(5.4538e-06), 'log': {'valid_loss': tensor(5.4538e-06)}, 'progress_bar': {'valid_loss': tensor(5.4538e-06)}}\n",
      "{'valid_loss': tensor(3.9935e-06), 'log': {'valid_loss': tensor(3.9935e-06)}, 'progress_bar': {'valid_loss': tensor(3.9935e-06)}}\n",
      "{'valid_loss': tensor(5.6996e-06), 'log': {'valid_loss': tensor(5.6996e-06)}, 'progress_bar': {'valid_loss': tensor(5.6996e-06)}}\n",
      "{'valid_loss': tensor(4.4852e-06), 'log': {'valid_loss': tensor(4.4852e-06)}, 'progress_bar': {'valid_loss': tensor(4.4852e-06)}}\n",
      "{'valid_loss': tensor(4.8056e-06), 'log': {'valid_loss': tensor(4.8056e-06)}, 'progress_bar': {'valid_loss': tensor(4.8056e-06)}}\n",
      "{'valid_loss': tensor(5.2750e-06), 'log': {'valid_loss': tensor(5.2750e-06)}, 'progress_bar': {'valid_loss': tensor(5.2750e-06)}}\n",
      "{'valid_loss': tensor(5.0813e-06), 'log': {'valid_loss': tensor(5.0813e-06)}, 'progress_bar': {'valid_loss': tensor(5.0813e-06)}}\n",
      "{'valid_loss': tensor(1.2137e-05), 'log': {'valid_loss': tensor(1.2137e-05)}, 'progress_bar': {'valid_loss': tensor(1.2137e-05)}}\n",
      "{'valid_loss': tensor(4.8354e-06), 'log': {'valid_loss': tensor(4.8354e-06)}, 'progress_bar': {'valid_loss': tensor(4.8354e-06)}}\n",
      "{'valid_loss': tensor(8.0838e-06), 'log': {'valid_loss': tensor(8.0838e-06)}, 'progress_bar': {'valid_loss': tensor(8.0838e-06)}}\n",
      "{'valid_loss': tensor(6.2659e-06), 'log': {'valid_loss': tensor(6.2659e-06)}, 'progress_bar': {'valid_loss': tensor(6.2659e-06)}}\n",
      "{'valid_loss': tensor(3.1888e-06), 'log': {'valid_loss': tensor(3.1888e-06)}, 'progress_bar': {'valid_loss': tensor(3.1888e-06)}}\n",
      "{'valid_loss': tensor(5.2526e-06), 'log': {'valid_loss': tensor(5.2526e-06)}, 'progress_bar': {'valid_loss': tensor(5.2526e-06)}}\n",
      "{'valid_loss': tensor(6.2062e-06), 'log': {'valid_loss': tensor(6.2062e-06)}, 'progress_bar': {'valid_loss': tensor(6.2062e-06)}}\n",
      "{'valid_loss': tensor(1.3664e-05), 'log': {'valid_loss': tensor(1.3664e-05)}, 'progress_bar': {'valid_loss': tensor(1.3664e-05)}}\n",
      "{'valid_loss': tensor(1.0475e-05), 'log': {'valid_loss': tensor(1.0475e-05)}, 'progress_bar': {'valid_loss': tensor(1.0475e-05)}}\n",
      "{'valid_loss': tensor(3.1292e-06), 'log': {'valid_loss': tensor(3.1292e-06)}, 'progress_bar': {'valid_loss': tensor(3.1292e-06)}}\n",
      "{'valid_loss': tensor(4.6417e-06), 'log': {'valid_loss': tensor(4.6417e-06)}, 'progress_bar': {'valid_loss': tensor(4.6417e-06)}}\n",
      "{'valid_loss': tensor(5.1111e-06), 'log': {'valid_loss': tensor(5.1111e-06)}, 'progress_bar': {'valid_loss': tensor(5.1111e-06)}}\n",
      "{'valid_loss': tensor(1.2904e-05), 'log': {'valid_loss': tensor(1.2904e-05)}, 'progress_bar': {'valid_loss': tensor(1.2904e-05)}}\n",
      "{'valid_loss': tensor(6.3106e-06), 'log': {'valid_loss': tensor(6.3106e-06)}, 'progress_bar': {'valid_loss': tensor(6.3106e-06)}}\n",
      "{'valid_loss': tensor(1.0334e-05), 'log': {'valid_loss': tensor(1.0334e-05)}, 'progress_bar': {'valid_loss': tensor(1.0334e-05)}}\n",
      "{'valid_loss': tensor(3.3304e-06), 'log': {'valid_loss': tensor(3.3304e-06)}, 'progress_bar': {'valid_loss': tensor(3.3304e-06)}}\n",
      "{'valid_loss': tensor(5.9306e-06), 'log': {'valid_loss': tensor(5.9306e-06)}, 'progress_bar': {'valid_loss': tensor(5.9306e-06)}}\n",
      "{'valid_loss': tensor(3.8966e-06), 'log': {'valid_loss': tensor(3.8966e-06)}, 'progress_bar': {'valid_loss': tensor(3.8966e-06)}}\n",
      "{'valid_loss': tensor(1.2658e-05), 'log': {'valid_loss': tensor(1.2658e-05)}, 'progress_bar': {'valid_loss': tensor(1.2658e-05)}}\n",
      "{'valid_loss': tensor(1.1354e-05), 'log': {'valid_loss': tensor(1.1354e-05)}, 'progress_bar': {'valid_loss': tensor(1.1354e-05)}}\n",
      "{'valid_loss': tensor(9.8718e-06), 'log': {'valid_loss': tensor(9.8718e-06)}, 'progress_bar': {'valid_loss': tensor(9.8718e-06)}}\n",
      "{'valid_loss': tensor(3.7280e-06), 'log': {'valid_loss': tensor(3.7280e-06)}, 'progress_bar': {'valid_loss': tensor(3.7280e-06)}}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:23: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## train roughly for about 10-15 minutes with GPU enabled.\n",
    "trainer = pl.Trainer(gpus=0, max_epochs=hparams.epochs, progress_bar_refresh_rate=10,\n",
    "                     accumulate_grad_batches=hparams.accumulate_grad_batches)\n",
    "\n",
    "trainer.fit(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2int = {\n",
    "  \"meta_minus_m\": 0,\n",
    "  \"meta_plus_m\": 1,\n",
    "  \"meta_zero\": 2,\n",
    "  \"meta_amb\": 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 2, does not match size of target_names, 4. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-4f5dcb27efa3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mpred_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel2int\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict)\u001b[0m\n\u001b[1;32m   1874\u001b[0m                 \u001b[0;34m\"Number of classes, {0}, does not match size of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1875\u001b[0m                 \u001b[0;34m\"target_names, {1}. Try specifying the labels \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1876\u001b[0;31m                 \u001b[0;34m\"parameter\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1877\u001b[0m             )\n\u001b[1;32m   1878\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of classes, 2, does not match size of target_names, 4. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    progress = [\"/\", \"-\", \"\\\\\", \"|\", \"/\", \"-\", \"\\\\\", \"|\"]\n",
    "    module.eval()\n",
    "    true_y, pred_y = [], []\n",
    "    for i, batch_ in enumerate(module.test_dataloader()):\n",
    "        X, y = batch_\n",
    "        batch = X\n",
    "        y_pred = torch.argmax(module(batch), dim=1)\n",
    "        true_y.extend(y)\n",
    "        pred_y.extend(y_pred)\n",
    "print(\"\\n\" + \"_\" * 80)\n",
    "print(classification_report(true_y, pred_y, target_names=label2int.keys(), digits=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
